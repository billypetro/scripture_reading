{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f76928d-450c-4cc1-8c69-15dded0fb3f2",
   "metadata": {},
   "source": [
    "### this loads my packages, preps/cleans the data, outputs 4 words n-grams, a wordcloud and important phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b312ba30-28a0-46e3-b2ce-398a55973f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1ab7192ba30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4 as bs #this will allow us to get the data from the webpage\n",
    "import pandas as pd #we love pandas!\n",
    "import urllib.request #this gets the link for us\n",
    "import re #this will help us with regular expressions\n",
    "import contractions # use this to get rid of contractions\n",
    "import string\n",
    "import nltk\n",
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams # function for making ngrams\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import numpy as np\n",
    "import sumy\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "source = urllib.request.urlopen('https://www.churchofjesuschrist.org/study/scriptures/nt/matt/15?lang=eng').read() #gives us the elements for the talk\n",
    "soup = bs.BeautifulSoup(source,'html') # this is the whole webpage\n",
    "#find and treat the speaker\n",
    "author = soup.find(\"p\", {'class':'author-name'})\n",
    "author = str(author)\n",
    "author = re.sub('<.*?>','',author)\n",
    "author= re.sub('By ', '', author)\n",
    "author= re.sub('\\.', '', author)\n",
    "author= re.sub(' ', '_', author)\n",
    "author = author.lower()\n",
    "#treat the title\n",
    "title = soup.find(\"title\")\n",
    "title = str(title)\n",
    "title = re.sub('<.*?>','',title)\n",
    "title = re.sub('\\xa0','_',title)\n",
    "title = re.sub(r'[^\\w\\s]','',title)\n",
    "title = title.replace(' ', '_')\n",
    "title = title.replace(':', '')\n",
    "title = title.lower()\n",
    "#grab the talk element\n",
    "talk = soup.find(\"div\", {\"class\": \"body-block\"})\n",
    "#unpack these data elements\n",
    "[x.extract() for x in talk('div')]\n",
    "[x.extract() for x in talk('sup')]\n",
    "[x.extract() for x in talk('span')]\n",
    "def remove_tags(html):\n",
    "    # parse html content\n",
    "    soup = talk \n",
    "    for data in soup('header'):\n",
    "        data.decompose()\n",
    "    for data in soup('sup class=\"marker\"'):\n",
    "        data.decompose()\n",
    "    for data in soup(['style', 'script']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "#one more cleanup of the text\n",
    "talk = remove_tags(talk)\n",
    "talk = re.sub('\\xa0',' ',talk)\n",
    "#remove possesive apostrophe\n",
    "talk = ' '.join(talk.replace(\"’s\", '').split())\n",
    "#remove possesive apostrophe\n",
    "talk = ' '.join(talk.replace(\"’\", '').split())\n",
    "#remove opening quote mark\n",
    "talk = ' '.join(talk.replace('“', '').split())\n",
    "#remove closing quote mark\n",
    "talk = ' '.join(talk.replace('”', '').split())\n",
    "#treat to remove contractions\n",
    "talk1=[]\n",
    "for word in talk.split():\n",
    "    talk1.append(contractions.fix(word)) \n",
    "talk = ' '.join(talk1).lower()\n",
    "#create the file for bigrams\n",
    "tokenized = talk.split()\n",
    "Bigrams = ngrams(tokenized, 4)\n",
    "# get the frequency of each bigram in our corpus\n",
    "BigramFreq = collections.Counter(Bigrams)\n",
    "# what are the ten most popular ngrams in this Spanish corpus?\n",
    "lists = BigramFreq.most_common(10)\n",
    "num = np.array(lists,dtype=object)\n",
    "reshaped = num.reshape(10,2)\n",
    "df = pd.DataFrame(reshaped, columns=['phrase','times appeared'])\n",
    "df['phrase'] = df['phrase'].apply(lambda x: ' '.join(x))\n",
    "file = open(\"{}.txt\".format(title), \"w\", encoding=\"utf-8\")  # append mode\n",
    "file.write(\"common phrases:\" + \"\\n\" + str(df) + \"\\n\")\n",
    "file.close()\n",
    "#parse the talk\n",
    "parser = PlaintextParser.from_string(talk,Tokenizer(\"italian\"))\n",
    "#summarize the talk - we choose 5 sentences here\n",
    "summarizer = LexRankSummarizer()\n",
    "summary = summarizer(parser.document, 5)\n",
    "summary1=[]\n",
    "for sentence in summary:\n",
    "    summary1.append(str(sentence))  \n",
    "num = np.array(summary1,dtype=object)\n",
    "reshaped = num.reshape(5)\n",
    "df = pd.DataFrame(reshaped, columns=[''])\n",
    "file = open(\"{}.txt\".format(title), \"a\", encoding = \"utf-8\")  # append mode\n",
    "file.write(\"important phrases:\" + \"\\n\"  +  str(df.values) + \"\\n\")\n",
    "file.close()\n",
    "# get rid of punctuation\n",
    "punctuationNoPeriod = \"[\" + re.sub(\"\\.\",\"\",string.punctuation) + \"]\"\n",
    "talk = re.sub(punctuationNoPeriod, \"\", talk)\n",
    "#get rid of possesives\n",
    "talk = ' '.join(talk.replace(\"’s\", '').split())\n",
    "# get rid of punctuation\n",
    "punctuationNoPeriod = \"[\" + re.sub(\"\\.\",\"\",string.punctuation) + \"]\"\n",
    "talk = re.sub(punctuationNoPeriod, \"\", talk)\n",
    "#remove words that are 3 or less letters - unless the word is god\n",
    "talk2=[]\n",
    "for w in talk.split():\n",
    "    #only words that are 4+ letters long\n",
    "    if len(w)>3:\n",
    "        #which are not stop_words\n",
    "        if w not in stop_words: \n",
    "            talk2.append(w)\n",
    "    if w in ['god']:\n",
    "        talk2.append(w)\n",
    "talk1 = ' '.join(talk2)\n",
    "talk1 = re.sub(r' +',r' ', talk1).strip()\n",
    "tokens = word_tokenize(talk1)\n",
    "tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "list_of_verbs = []\n",
    "for i in range(len(tags)):\n",
    "    if tags[i][1] in ['ADV','DET','ADJ','NOUN','VERB','X']:\n",
    "        list_of_verbs.append(tags[i][0])\n",
    "    # Import the wordcloud library\n",
    "    from wordcloud import WordCloud\n",
    "new_verbs=[]\n",
    "for i in list_of_verbs:\n",
    "    new_verbs.append(lemmatizer.lemmatize(i))\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(new_verbs)\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=20, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_file(\"{}.png\".format(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb93a44d-38fb-446d-b9da-7e008bc3af4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
